{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bffarinha/Melanoma_PFG/blob/main/PFG_melanoma_version_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRp9tT5prwrI"
      },
      "source": [
        "# Melanoma detection in dermatoscopic images using contextual information and Convolutional Neural Networks.\n",
        "By Brenda Farinha Fernandes\n",
        "\n",
        "November 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Trained using TensorFlow Keras 2.11 on Google Colaboratory TPUs\n",
        "*   All images are contained within a TFRecord\n",
        "*   All TFRecords contain similar proportions of benign/malign cases\n",
        "*   Images in TFRecord format have been resized to a uniform 1024x1024.\n",
        "\n"
      ],
      "metadata": {
        "id": "BlATKVs6nuw3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y8KGpDKryXe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L0ur1R8r1Mn"
      },
      "source": [
        "### Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd7pZ0R2r1mw",
        "outputId": "635ad92e-b050-4555-8776-a235970e80aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version  2.11.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow version ', tf.__version__)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQP3UYuSr9VL"
      },
      "source": [
        "### TPU or GPU Distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbuJk4jYsBkz",
        "outputId": "ce886a3f-98d2-4c10-a451-57dd9243fa45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU grpc://10.75.131.50:8470\n",
            "Running on TPU  ['10.75.131.50:8470']\n",
            "REPLICAS: 8\n"
          ]
        }
      ],
      "source": [
        "# Detect hardware \n",
        "try: \n",
        "  # TPU detection\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  print('Running on TPU', tpu.master())\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "  print('Not connect to TPU')\n",
        "\n",
        "# Select appropriate distribution strategy \n",
        "if tpu: \n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "elif len(gpus) > 1:\n",
        "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "elif len(gpus) == 1:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "print('REPLICAS:', REPLICAS) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zij54pFdszZi"
      },
      "source": [
        "### Global parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfHBvZuisyOV",
        "outputId": "a89dfa65-c296-45d5-cc9d-224964aec167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH SIZE: 128\n",
            "EPOCHS 100\n",
            "IMAGE SIZE: 256 x 256\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16 * REPLICAS\n",
        "BUFFER_SIZE = 2048\n",
        "EPOCHS = 100\n",
        "IMAGE_SIZE = [256, 256]\n",
        "\n",
        "print(\"BATCH SIZE:\", BATCH_SIZE)\n",
        "print(\"BUFFER SIZE:\", BUFFER_SIZE)\n",
        "print(\"EPOCHS\", EPOCHS)\n",
        "print(\"IMAGE SIZE:\", IMAGE_SIZE[0],\"x\",IMAGE_SIZE[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3J4gUMms1H7"
      },
      "source": [
        "Paths of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fuKsWl0Cs22b"
      },
      "outputs": [],
      "source": [
        "PATH_ds_img_jpeg_zip = '/content/drive/MyDrive/PFG_MELANOMA/ISIC_2020_Training_JPEG.zip'\n",
        "PATH_ds_img_tfrecords_zip = '/content/drive/MyDrive/PFG_MELANOMA/tfrecords.zip'\n",
        "\n",
        "PATH_ds_tfrecords = '/content/drive/MyDrive/PFG_MELANOMA/tfrecords'\n",
        "PATH_ds_jpeg = '/content/drive/MyDrive/PFG_MELANOMA/images'\n",
        "\n",
        "PATH_ds_train = '/content/drive/MyDrive/PFG_MELANOMA/ISIC_2020_Training_GroundTruth.csv'\n",
        "\n",
        "GCS_PATH_TRAIN = 'gs://ds-tfrecord/train-tfrecord/train*.tfrec'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRpBPJwqOv3y"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF2WOyqiL7jB"
      },
      "source": [
        "Read [SIIM-ISIC 2020 Melanoma Classification Challenge Dataset](https://doi.org/10.34970/2020-ds01) \n",
        "\n",
        "Images are also provided in JPEG and TFRecord format (in the jpeg and tfrecords directories, respectively). Images in TFRecord format have been resized to a uniform 1024x1024.\n",
        "\n",
        "Metadata is also provided outside of the DICOM format, in CSV files with this description:\n",
        "\n",
        "* `image_name` - unique identifier, points to filename of related DICOM image\n",
        "* `patient_id` - unique patient identifier\n",
        "* `sex` - the sex of the patient (when unknown, will be blank)\n",
        "* `age_approx` - approximate patient age at time of imaging\n",
        "* `anatom_site_general_challenge` - location of imaged site\n",
        "* `diagnosis` - detailed diagnosis information\n",
        "* `benign_malignant` - indicator of malignancy of imaged lesion\n",
        "* `target` - binarized version of the target variable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lxz5mPUfs8OZ"
      },
      "outputs": [],
      "source": [
        "# Unzip dataset from Google Drive\n",
        "# For TPU version not use \n",
        "dataset_unzip = False\n",
        "if dataset_unzip: \n",
        "  zipfile.ZipFile(PATH_ds_img_jpeg_zip).extractall(PATH_ds_jpeg)\n",
        "  zipfile.ZipFile(PATH_ds_img_tfrecords_zip).extractall(PATH_ds_tfrecords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MS4-MqV_tJPt",
        "outputId": "e225f975-1ed1-41a2-c50e-b9a428b44daa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     image_name  patient_id     sex  age_approx anatom_site_general_challenge  \\\n",
              "0  ISIC_2637011  IP_7279968    male        45.0                     head/neck   \n",
              "1  ISIC_0015719  IP_3075186  female        45.0               upper extremity   \n",
              "2  ISIC_0052212  IP_2842074  female        50.0               lower extremity   \n",
              "3  ISIC_0068279  IP_6890425  female        45.0                     head/neck   \n",
              "4  ISIC_0074268  IP_8723313  female        55.0               upper extremity   \n",
              "\n",
              "  diagnosis benign_malignant  target  \n",
              "0   unknown           benign       0  \n",
              "1   unknown           benign       0  \n",
              "2     nevus           benign       0  \n",
              "3   unknown           benign       0  \n",
              "4   unknown           benign       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d087813d-1d90-4ce4-8bce-304d773753c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>sex</th>\n",
              "      <th>age_approx</th>\n",
              "      <th>anatom_site_general_challenge</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>benign_malignant</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_2637011</td>\n",
              "      <td>IP_7279968</td>\n",
              "      <td>male</td>\n",
              "      <td>45.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_0015719</td>\n",
              "      <td>IP_3075186</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_0052212</td>\n",
              "      <td>IP_2842074</td>\n",
              "      <td>female</td>\n",
              "      <td>50.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>nevus</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_0068279</td>\n",
              "      <td>IP_6890425</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0074268</td>\n",
              "      <td>IP_8723313</td>\n",
              "      <td>female</td>\n",
              "      <td>55.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>unknown</td>\n",
              "      <td>benign</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d087813d-1d90-4ce4-8bce-304d773753c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d087813d-1d90-4ce4-8bce-304d773753c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d087813d-1d90-4ce4-8bce-304d773753c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Read dataset CSV with metadata information\n",
        "train = pd.read_csv(f'{PATH_ds_train}')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yjHmslpnUbOd"
      },
      "outputs": [],
      "source": [
        "# Count images\n",
        "def count_data_images(path):\n",
        "  # The number of data images is written in the name of the .tfrec files. Example: train10-2071.tfrec = 2071 data items\n",
        "  n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in path]\n",
        "  return np.sum(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAL0PVCUAUgy",
        "outputId": "e7e13e88-2478-43bd-c110-7a9767308b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training files =  14\n",
            "Number of validation files =  2\n"
          ]
        }
      ],
      "source": [
        "# Split dataset for training and validation \n",
        "train_files, val_files = train_test_split(tf.io.gfile.glob(GCS_PATH_TRAIN), test_size=0.1, random_state=42, shuffle=True)\n",
        "\n",
        "print(\"Number of training files = \", len(train_files))\n",
        "print(\"Number of validation files = \", len(val_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTSxPf6L3n-1",
        "outputId": "0cad667d-9c0d-456e-c416-b2cc0ccc1a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training Images = 28984\n",
            "Numer of steps per epoch in Train = 226\n",
            "\n",
            "Number of validation Images = 4142\n",
            "Numer of steps per epoch in Train = 32\n"
          ]
        }
      ],
      "source": [
        "num_training_images = count_data_images(train_files)\n",
        "num_validation_images = count_data_images(val_files)\n",
        "\n",
        "STEPS_PER_EPOCH_TRAIN = num_training_images // BATCH_SIZE\n",
        "STEPS_PER_EPOCH_VAL = num_validation_images // BATCH_SIZE\n",
        "\n",
        "print(\"Number of training Images =\", num_training_images)\n",
        "print(\"Numer of steps per epoch in Train =\", STEPS_PER_EPOCH_TRAIN)\n",
        "print(\"\\nNumber of validation Images =\", num_validation_images)\n",
        "print(\"Numer of steps per epoch in Train =\", STEPS_PER_EPOCH_VAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYlvcl9RtVca"
      },
      "source": [
        "## Data preproccesing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0wYNilfCta1v"
      },
      "outputs": [],
      "source": [
        "def decode_image(image):\n",
        "\n",
        "  image = tf.image.decode_jpeg(image, channels = 3)\n",
        "  image = tf.cast(image, tf.float32) / 255.0\n",
        "  image = tf.image.resize(image, IMAGE_SIZE)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mn0Dhw0Xtudr"
      },
      "outputs": [],
      "source": [
        "def read_labeled_tfrecord(example, labeled = False):\n",
        "\n",
        "  # Create a description of the features.\n",
        "  if labeled: \n",
        "    TFREC_FORMAT = {\n",
        "        \"image\"       : tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\"      : tf.io.FixedLenFeature([], tf.int64)     \n",
        "    }\n",
        "  else: \n",
        "    TFREC_FORMAT = {\n",
        "        \"image\"       : tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image_name\"  : tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "\n",
        "  example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "  \n",
        "  image = decode_image(example[\"image\"])\n",
        "  \n",
        "  if labeled: \n",
        "    label = tf.cast(example[\"target\"], tf.int32)\n",
        "    return image, label\n",
        "  else:\n",
        "    image_name = example[\"image_name\"]\n",
        "    return image, image_name   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a0HCdwjyGJ7B"
      },
      "outputs": [],
      "source": [
        "def image_augmentation(image, label):\n",
        "\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  image = tf.image.random_flip_up_down(image)\n",
        "  image = tf.image.random_hue(image, 0.025)\n",
        "  image = tf.image.random_saturation(image, 0.6, 1.4)\n",
        "  image = tf.image.random_contrast(image, 0.7, 1.4)\n",
        "  image = tf.image.random_brightness(image, 0.1)\n",
        "\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Uwyw2KXBDO"
      },
      "source": [
        "[tf.data.TFRecordDataset Methods](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset#methods_2)\n",
        "\n",
        "* `tf.data.Options():` An Options object can be, for instance, used to control which graph optimizations to apply.\n",
        "\n",
        "* `.experiemental_deterministic:` experimental_deterministic refers to whether the outputs need to be produced in deterministic order. If None, defaults to True. Here, the data is unordered, hence we don't need to process it in an order which may slow down our speed.\n",
        "\n",
        "* `cache():` Caches the elements in this dataset. The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\n",
        "\n",
        "* `repeat():` Repeats this dataset so each original value is seen count times.\n",
        "\n",
        "* `shuffle():` Randomly shuffles the elements of this dataset. This dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
        "\n",
        "* `map():` Maps map_func across the elements of this dataset. This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l_JbWP_-1FaN"
      },
      "outputs": [],
      "source": [
        "def load_dataset(files, ordered = False, labeled = False, repeat = False, cache = False, augment = False): \n",
        "\n",
        "  ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTOTUNE)\n",
        "\n",
        "  if not ordered: \n",
        "    ds = ds.shuffle(1024)\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_deterministic = False\n",
        "    ds = ds.with_options(options)\n",
        "  \n",
        "  ds = ds.map(partial(read_labeled_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n",
        "  #ds = ds.map(lambda example: read_labeled_tfrecord(example, labeled), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  if augment: \n",
        "    ds = ds.map(image_augmentation, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  if repeat: \n",
        "    ds = ds.repeat()\n",
        "\n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "  if cache: \n",
        "    ds = ds.cache()\n",
        "\n",
        "  ds = ds.prefetch(AUTOTUNE)\n",
        "\n",
        "  return ds  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x6okRJ6l2Q3P"
      },
      "outputs": [],
      "source": [
        "train_ds = load_dataset(train_files, ordered = False, labeled = True, repeat = True, cache = False, augment = True)\n",
        "val_ds = load_dataset(val_files, ordered = False, labeled = True, repeat = False, cache = True, augment = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ds = tf.data.TFRecordDataset(train_files, num_parallel_reads=AUTOTUNE)"
      ],
      "metadata": {
        "id": "-OhFcVCtDO7F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#raw_example = next(iter(ds))\n",
        "#parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "#parsed.features"
      ],
      "metadata": {
        "id": "uqv_9HVHDRfE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJiWK2TD4D_7"
      },
      "source": [
        "## Model fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB8IzOWl4HNz"
      },
      "source": [
        "### [Calculate class weights ](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#calculate_class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "revisar pq estos no son los pesos correctos. dividi el train en val y train. sacar este dato de los tfrecord\n"
      ],
      "metadata": {
        "id": "e5UKOq0zO_hX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXjX2JtU4GrZ",
        "outputId": "4ed3b995-bda5-43b0-8a83-1b68bc46fe4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Malignant Cases in Train Data =  583\n",
            "Benign Cases In Train Dataset =  32542\n",
            "Total Cases In Train Dataset =  33125\n",
            "Ratio of Malignant to Benign =  0.017915309446254073\n"
          ]
        }
      ],
      "source": [
        "malignant = len(train[train[\"target\"] == 1])\n",
        "benign = len(train[train[\"target\"] == 0 ])\n",
        "total = len(train) \n",
        "\n",
        "print(\"Malignant Cases in Train Data = \", malignant)\n",
        "print(\"Benign Cases In Train Dataset = \",benign)\n",
        "print(\"Total Cases In Train Dataset = \",total)\n",
        "print(\"Ratio of Malignant to Benign = \",malignant/benign)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjB2AnOc4Wnu",
        "outputId": "a4e86c55-3b01-4bc8-f02e-9b8c540dffce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for benign cases = 0.51 \n",
            "Weight for malignant cases = 28.41 \n"
          ]
        }
      ],
      "source": [
        "weight_malignant = (total/malignant)/2.0\n",
        "weight_benign = (total/benign)/2.0\n",
        "\n",
        "class_weight = {0 : weight_benign , 1 : weight_malignant}\n",
        "\n",
        "print(\"Weight for benign cases = {:.2f} \" .format(class_weight[0]))\n",
        "print(\"Weight for malignant cases = {:.2f} \" .format(class_weight[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfuOqJ0p4cxG"
      },
      "source": [
        "Defining Callbacks\n",
        "\n",
        "*   `EarlyStopping`: Stop training when a monitored metric has stopped improving.\n",
        "*   `ModelCheckpoint`: Callback to save the Keras model or model weights at some frequency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wJMaYSnR4cxH"
      },
      "outputs": [],
      "source": [
        "def get_lr_callback():\n",
        "\n",
        "  callback_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "      patience = 15, \n",
        "      verbose = 0, \n",
        "      restore_best_weights = True)\n",
        "\n",
        "  callbacks_lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "      monitor = \"val_auc\", \n",
        "      factor = 0.1, \n",
        "      patience = 10,\n",
        "      verbose = 0, \n",
        "      min_lr = 1e-6)\n",
        "\n",
        "  callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"melanoma_detection_weights.h5\",\n",
        "      save_weights_only=True,\n",
        "      monitor='val_auc',\n",
        "      mode='max',\n",
        "      save_best_only=True,\n",
        "      verbose=1)\n",
        "  \n",
        "  return [callback_early_stopping, callbacks_lr_reduce, callback_checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model():\n",
        "  with strategy.scope():\n",
        "\n",
        "    bias = np.log(malignant/benign)\n",
        "    bias = tf.keras.initializers.Constant(bias)\n",
        "  \n",
        "    input = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
        "\n",
        "    cnn_model = tf.keras.applications.EfficientNetB6(\n",
        "        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )(input)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(cnn_model) \n",
        "\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=bias)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=input, outputs=output, name='aNetwork')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.05),\n",
        "        metrics = [tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "P8q2L02JUqZn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_model = make_model()\n",
        "\n",
        "initial_model.fit(\n",
        "    train_ds, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=True, \n",
        "    steps_per_epoch=STEPS_PER_EPOCH_TRAIN, \n",
        "    validation_data=val_ds, \n",
        "    validation_steps=STEPS_PER_EPOCH_VAL,\n",
        "    callbacks=get_lr_callback(),\n",
        "    class_weight = class_weight\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsKdO_yFib-E",
        "outputId": "b2ac099f-5ec0-4cbf-b528-c1a8bba16053"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"aNetwork\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb6 (Functional)  (None, 8, 8, 2304)       40960143  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2304)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 2305      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,962,448\n",
            "Trainable params: 40,738,009\n",
            "Non-trainable params: 224,439\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7160 - auc: 0.6937\n",
            "Epoch 1: val_auc improved from -inf to 0.60056, saving model to melanoma_detection_weights.h5\n",
            "226/226 [==============================] - 366s 535ms/step - loss: 0.7160 - auc: 0.6937 - val_loss: 0.2001 - val_auc: 0.6006 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5920 - auc: 0.7627\n",
            "Epoch 2: val_auc improved from 0.60056 to 0.76776, saving model to melanoma_detection_weights.h5\n",
            "226/226 [==============================] - 86s 380ms/step - loss: 0.5920 - auc: 0.7627 - val_loss: 0.2660 - val_auc: 0.7678 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5805 - auc: 0.7771\n",
            "Epoch 3: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 78s 347ms/step - loss: 0.5805 - auc: 0.7771 - val_loss: 0.2140 - val_auc: 0.6329 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5852 - auc: 0.7736\n",
            "Epoch 4: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 81s 358ms/step - loss: 0.5852 - auc: 0.7736 - val_loss: 0.3819 - val_auc: 0.6203 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5864 - auc: 0.7673\n",
            "Epoch 5: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 77s 342ms/step - loss: 0.5864 - auc: 0.7673 - val_loss: 0.2000 - val_auc: 0.5675 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5811 - auc: 0.7789\n",
            "Epoch 6: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 80s 355ms/step - loss: 0.5811 - auc: 0.7789 - val_loss: 0.1833 - val_auc: 0.3770 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5751 - auc: 0.7806\n",
            "Epoch 7: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 84s 372ms/step - loss: 0.5751 - auc: 0.7806 - val_loss: 0.1820 - val_auc: 0.5836 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5959 - auc: 0.7686\n",
            "Epoch 8: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 74s 325ms/step - loss: 0.5959 - auc: 0.7686 - val_loss: 0.5250 - val_auc: 0.6350 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5739 - auc: 0.7838\n",
            "Epoch 9: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 76s 335ms/step - loss: 0.5739 - auc: 0.7838 - val_loss: 1.5590 - val_auc: 0.7166 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5832 - auc: 0.7811\n",
            "Epoch 10: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 79s 352ms/step - loss: 0.5832 - auc: 0.7811 - val_loss: 0.5667 - val_auc: 0.7001 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5503 - auc: 0.8023\n",
            "Epoch 11: val_auc did not improve from 0.76776\n",
            "226/226 [==============================] - 72s 319ms/step - loss: 0.5503 - auc: 0.8023 - val_loss: 0.4749 - val_auc: 0.6909 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5574 - auc: 0.8017\n",
            "Epoch 12: val_auc improved from 0.76776 to 0.78083, saving model to melanoma_detection_weights.h5\n",
            "226/226 [==============================] - 79s 351ms/step - loss: 0.5574 - auc: 0.8017 - val_loss: 0.4075 - val_auc: 0.7808 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5535 - auc: 0.8050\n",
            "Epoch 13: val_auc did not improve from 0.78083\n",
            "226/226 [==============================] - 81s 359ms/step - loss: 0.5535 - auc: 0.8050 - val_loss: 0.1726 - val_auc: 0.7151 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5503 - auc: 0.8099\n",
            "Epoch 14: val_auc did not improve from 0.78083\n",
            "226/226 [==============================] - 69s 304ms/step - loss: 0.5503 - auc: 0.8099 - val_loss: 0.5021 - val_auc: 0.6718 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5574 - auc: 0.7965\n",
            "Epoch 15: val_auc did not improve from 0.78083\n",
            "226/226 [==============================] - 72s 320ms/step - loss: 0.5574 - auc: 0.7965 - val_loss: 0.2888 - val_auc: 0.7095 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5375 - auc: 0.8196\n",
            "Epoch 16: val_auc did not improve from 0.78083\n",
            "226/226 [==============================] - 75s 331ms/step - loss: 0.5375 - auc: 0.8196 - val_loss: 0.3204 - val_auc: 0.6775 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5095 - auc: 0.8413\n",
            "Epoch 17: val_auc improved from 0.78083 to 0.79848, saving model to melanoma_detection_weights.h5\n",
            "226/226 [==============================] - 81s 359ms/step - loss: 0.5095 - auc: 0.8413 - val_loss: 0.4800 - val_auc: 0.7985 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5167 - auc: 0.8389\n",
            "Epoch 18: val_auc improved from 0.79848 to 0.83796, saving model to melanoma_detection_weights.h5\n",
            "226/226 [==============================] - 74s 327ms/step - loss: 0.5167 - auc: 0.8389 - val_loss: 0.4968 - val_auc: 0.8380 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5061 - auc: 0.8459\n",
            "Epoch 19: val_auc improved from 0.83796 to 0.84175, saving model to melanoma_detection_weights.h5\n",
            "226/226 [==============================] - 77s 340ms/step - loss: 0.5061 - auc: 0.8459 - val_loss: 0.4630 - val_auc: 0.8417 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4968 - auc: 0.8554\n",
            "Epoch 20: val_auc did not improve from 0.84175\n",
            "226/226 [==============================] - 76s 338ms/step - loss: 0.4968 - auc: 0.8554 - val_loss: 0.4158 - val_auc: 0.8182 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5043 - auc: 0.8499\n",
            "Epoch 21: val_auc did not improve from 0.84175\n",
            "226/226 [==============================] - 79s 347ms/step - loss: 0.5043 - auc: 0.8499 - val_loss: 0.4692 - val_auc: 0.8335 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4847 - auc: 0.8640\n",
            "Epoch 22: val_auc did not improve from 0.84175\n",
            "226/226 [==============================] - 71s 315ms/step - loss: 0.4847 - auc: 0.8640 - val_loss: 0.3730 - val_auc: 0.8398 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4976 - auc: 0.8560\n",
            "Epoch 23: val_auc did not improve from 0.84175\n",
            "226/226 [==============================] - 74s 328ms/step - loss: 0.4976 - auc: 0.8560 - val_loss: 0.4083 - val_auc: 0.8350 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4820 - auc: 0.8658\n",
            "Epoch 24: val_auc did not improve from 0.84175\n",
            "226/226 [==============================] - 76s 335ms/step - loss: 0.4820 - auc: 0.8658 - val_loss: 0.3725 - val_auc: 0.8219 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4746 - auc: 0.8701\n",
            "Epoch 25: val_auc improved from 0.84175 to 0.84933, saving model to melanoma_detection_weights.h5\n",
            "226/226 [==============================] - 83s 367ms/step - loss: 0.4746 - auc: 0.8701 - val_loss: 0.4112 - val_auc: 0.8493 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4909 - auc: 0.8605\n",
            "Epoch 26: val_auc did not improve from 0.84933\n",
            "226/226 [==============================] - 72s 318ms/step - loss: 0.4909 - auc: 0.8605 - val_loss: 0.4222 - val_auc: 0.8375 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4781 - auc: 0.8714\n",
            "Epoch 27: val_auc did not improve from 0.84933\n",
            "226/226 [==============================] - 74s 328ms/step - loss: 0.4781 - auc: 0.8714 - val_loss: 0.5156 - val_auc: 0.8370 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4647 - auc: 0.8808\n",
            "Epoch 28: val_auc did not improve from 0.84933\n",
            "226/226 [==============================] - 96s 423ms/step - loss: 0.4647 - auc: 0.8808 - val_loss: 0.5166 - val_auc: 0.8380 - lr: 1.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fab9c395f70>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_weights = \"melanoma_detection_weights.h5\"\n",
        "initial_model.save_weights(initial_weights)"
      ],
      "metadata": {
        "id": "TYrRFoFzU3VG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weighted_model = make_model()\n",
        "#weighted_model.load_weights(initial_weights)"
      ],
      "metadata": {
        "id": "af_X2vSUcWAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate metrics"
      ],
      "metadata": {
        "id": "JmUMFyiFe-lp"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "9L0ur1R8r1Mn",
        "NQP3UYuSr9VL",
        "zRpBPJwqOv3y"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMjefJjElFsSEdM5cq5o/m1",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}